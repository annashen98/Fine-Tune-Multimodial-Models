{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5518bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0dec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_txt = \"captions.txt\"     # ä½ çš„åŸå§‹txtæ–‡ä»¶å\n",
    "output_jsonl = \"captions.jsonl\"  # è¾“å‡ºçš„jsonlæ–‡ä»¶å\n",
    "\n",
    "with open(input_txt, \"r\", encoding=\"utf-8\") as fin, open(output_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        # æ‹†åˆ†ä¸ºæ–‡ä»¶åå’Œæè¿°\n",
    "        if ':' in line:\n",
    "            image_name, caption = line.split(':', 1)\n",
    "            image_name = image_name.strip()\n",
    "            caption = caption.strip()\n",
    "            # ç»„è£…æˆjsonå¹¶å†™å…¥\n",
    "            json.dump({\"image\": image_name, \"caption\": caption}, fout, ensure_ascii=False)\n",
    "            fout.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a55679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cpu\n",
      "4.40.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "# import peft\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)\n",
    "# print(peft.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a567a29",
   "metadata": {},
   "source": [
    "## åˆ’åˆ†è®­ç»ƒé›†éªŒè¯é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387ae094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†å†™å…¥ C:/Users/taste/Documents/0_sis/captions_train.jsonlï¼Œå…± 160 æ¡\n",
      "éªŒè¯é›†å†™å…¥ C:/Users/taste/Documents/0_sis/captions_val.jsonlï¼Œå…± 40 æ¡\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "input_file = \"C:/Users/taste/Documents/0_sis/captions.jsonl\"\n",
    "train_file = \"C:/Users/taste/Documents/0_sis/captions_train.jsonl\"\n",
    "val_file = \"C:/Users/taste/Documents/0_sis/captions_val.jsonl\"\n",
    "\n",
    "# è¯»å…¥æ‰€æœ‰æ•°æ®\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "    data = [json.loads(line) for line in fin if line.strip()]\n",
    "\n",
    "# æ‰“ä¹±é¡ºåº\n",
    "random.shuffle(data)\n",
    "\n",
    "# æŒ‰8:2åˆ†\n",
    "split_idx = int(len(data) * 0.8)\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "# å†™å…¥æ–°æ–‡ä»¶\n",
    "with open(train_file, \"w\", encoding=\"utf-8\") as ftrain:\n",
    "    for item in train_data:\n",
    "        ftrain.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(val_file, \"w\", encoding=\"utf-8\") as fval:\n",
    "    for item in val_data:\n",
    "        fval.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å†™å…¥ {train_file}ï¼Œå…± {len(train_data)} æ¡\")\n",
    "print(f\"éªŒè¯é›†å†™å…¥ {val_file}ï¼Œå…± {len(val_data)} æ¡\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b20a5e",
   "metadata": {},
   "source": [
    "## å¼€å§‹å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04739f07",
   "metadata": {},
   "source": [
    "### ç¬¬ä¸€æ­¥ï¼ŒåŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f947d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taste\\Documents\\0_sis\\.florence2env2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2ed5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½å‡flash_attn\n",
    "import sys, types, importlib.machinery\n",
    "\n",
    "def make_fake_mod(name):\n",
    "    mod = types.ModuleType(name)\n",
    "    mod.__spec__ = importlib.machinery.ModuleSpec(name, None)\n",
    "    return mod\n",
    "\n",
    "sys.modules[\"flash_attn\"] = make_fake_mod(\"flash_attn\")\n",
    "sys.modules[\"flash_attn.ops\"] = make_fake_mod(\"flash_attn.ops\")\n",
    "sys.modules[\"flash_attn.bert_flash_attention\"] = make_fake_mod(\"flash_attn.bert_flash_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e1f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import csv\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fec3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Florence2CaptionDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, image_dir):\n",
    "        self.samples = []\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    item = json.loads(line)\n",
    "                    self.samples.append(item)\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        image_path = os.path.join(self.image_dir, item[\"image\"])\n",
    "        image = Image.open(image_path)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        prompt = \"<DETAILED_CAPTION>\"\n",
    "        caption = item[\"caption\"]\n",
    "        return {\"prompt\": prompt, \"caption\": caption, \"image\": image}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13086c",
   "metadata": {},
   "source": [
    "è®­ç»ƒä»è¿™å¼€å§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43117415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taste\\Documents\\0_sis\\.florence2env2\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½florence-2-baseæ¨¡å‹\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "CKPT = \"microsoft/Florence-2-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CKPT, trust_remote_code=True\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    CKPT, trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a76f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€åŒ–ç‰ˆæœ¬\n",
    "def collate_fn(batch):\n",
    "    prompts = [b[\"prompt\"] for b in batch]\n",
    "    captions = [b[\"caption\"] for b in batch]\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    model_inputs = processor(\n",
    "        text=prompts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    labels = processor(\n",
    "        text=captions,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return {k: v.to(model.device) for k, v in model_inputs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "814b6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "train_jsonl = r\"C:\\Users\\taste\\Documents\\0_sis\\captions_train.jsonl\"\n",
    "val_jsonl = r\"C:\\Users\\taste\\Documents\\0_sis\\captions_val.jsonl\"\n",
    "img_dir = r\"C:\\Users\\taste\\Documents\\0_sis\\processed\"\n",
    "\n",
    "train_ds = Florence2CaptionDataset(train_jsonl, img_dir)\n",
    "val_ds = Florence2CaptionDataset(val_jsonl, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1043e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1. å‚æ•°ç½‘æ ¼ ====\n",
    "param_grid = [\n",
    "    #{\"BATCH_SIZE\": 4, \"LEARNING_RATE\": 5e-6, \"EPOCHS\": 15},\n",
    "    #{\"BATCH_SIZE\": 4, \"LEARNING_RATE\": 2e-5, \"EPOCHS\": 15},\n",
    "    #{\"BATCH_SIZE\": 8, \"LEARNING_RATE\": 5e-6, \"EPOCHS\": 15},\n",
    "    #{\"BATCH_SIZE\": 2, \"LEARNING_RATE\": 1e-5, \"EPOCHS\": 15},\n",
    "    {\"BATCH_SIZE\": 4, \"LEARNING_RATE\": 1e-5, \"EPOCHS\": 30},\n",
    "]\n",
    "\n",
    "# ==== 2. LoRA é…ç½® ====\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# ==== 3. æ¨ç†å‡½æ•°ï¼ˆæ¯ç»„å‚æ•°åå¯ä»¥ç”¨æ¥è¯„ä¼°æ•ˆæœï¼‰====\n",
    "def infer_caption(model, processor, img_path, device):\n",
    "    from PIL import Image\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = \"<DETAILED_CAPTION>\"\n",
    "    inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            pixel_values=inputs.pixel_values,\n",
    "            max_new_tokens=80,\n",
    "            num_beams=3\n",
    "        )\n",
    "    output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(f\"å›¾ç‰‡ {img_path} ç”Ÿæˆæè¿°ï¼š{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f81cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 4. åˆå§‹åŒ–CSV ====\n",
    "loss_csv_path = \"train_val_loss_log_5.csv\"\n",
    "with open(loss_csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"param_group\", \"epoch\", \"train_loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ace287",
   "metadata": {},
   "source": [
    "### æ­£å¼å¼€å§‹å¾ªç¯è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afbe8df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== å®éªŒå‚æ•°: BATCH_SIZE=4, LR=1e-05, EPOCHS=30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taste\\Documents\\0_sis\\.florence2env2\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,929,928 || all params: 233,343,944 || trainable%: 0.8270743893829102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 1 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:16<00:00, 15.40s/it, avg_loss=5.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train avg loss: 5.6013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.36s/it]\n",
      "c:\\Users\\taste\\Documents\\0_sis\\.florence2env2\\lib\\site-packages\\peft\\utils\\save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val avg loss: 5.5625\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch01\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 2 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:18<00:00, 15.46s/it, avg_loss=5.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train avg loss: 5.3160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val avg loss: 5.2323\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch02\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 3 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:56<00:00, 14.91s/it, avg_loss=4.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train avg loss: 5.0171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val avg loss: 4.7568\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch03\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 4 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:45<00:00, 14.63s/it, avg_loss=4.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train avg loss: 4.5821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val avg loss: 4.2796\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch04\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 5 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:04<00:00, 15.10s/it, avg_loss=3.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train avg loss: 4.1129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val avg loss: 3.8380\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch05\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 6 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:12<00:00, 15.32s/it, avg_loss=3.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train avg loss: 3.7389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val avg loss: 3.5352\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch06\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 7 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:12<00:00, 13.82s/it, avg_loss=3.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train avg loss: 3.4814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val avg loss: 3.3356\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch07\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 8 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:05<00:00, 15.15s/it, avg_loss=3.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train avg loss: 3.2656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val avg loss: 3.1635\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch08\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 9 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:08<00:00, 15.21s/it, avg_loss=2.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train avg loss: 3.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val avg loss: 3.0044\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch09\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 10 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:34<00:00, 14.37s/it, avg_loss=2.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train avg loss: 2.9508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val avg loss: 2.8584\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch10\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 11 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:46<00:00, 14.65s/it, avg_loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train avg loss: 2.8235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Val avg loss: 2.7054\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch11\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 12 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:21<00:00, 15.54s/it, avg_loss=2.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train avg loss: 2.7040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Val avg loss: 2.5726\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch12\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 13 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:23<00:00, 15.58s/it, avg_loss=2.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train avg loss: 2.5813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Val avg loss: 2.4395\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch13\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 14 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:19<00:00, 15.48s/it, avg_loss=2.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train avg loss: 2.4465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Val avg loss: 2.3307\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch14\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 15 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:10<00:00, 15.27s/it, avg_loss=2.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train avg loss: 2.3590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Val avg loss: 2.2377\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch15\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 16 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:03<00:00, 15.10s/it, avg_loss=2.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Train avg loss: 2.2483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Val avg loss: 2.1664\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch16\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 17 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:45<00:00, 14.64s/it, avg_loss=2.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Train avg loss: 2.1964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Val avg loss: 2.1149\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch17\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 18 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:11<00:00, 15.28s/it, avg_loss=2.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Train avg loss: 2.1422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Val avg loss: 2.0747\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch18\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 19 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:06<00:00, 15.17s/it, avg_loss=1.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Train avg loss: 2.0999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Val avg loss: 2.0451\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch19\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 20 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:14<00:00, 15.36s/it, avg_loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Train avg loss: 2.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Val avg loss: 2.0220\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch20\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 21 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:36<00:00, 14.41s/it, avg_loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Train avg loss: 2.0572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Val avg loss: 2.0037\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch21\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 22 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:34<00:00, 14.36s/it, avg_loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Train avg loss: 1.9957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Val avg loss: 1.9892\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch22\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 23 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:36<00:00, 14.42s/it, avg_loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Train avg loss: 1.9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Val avg loss: 1.9771\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch23\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 24 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:22<00:00, 14.07s/it, avg_loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Train avg loss: 1.9715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Val avg loss: 1.9677\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch24\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 25 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:23<00:00, 14.08s/it, avg_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Train avg loss: 1.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Val avg loss: 1.9595\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch25\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 26 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:50<00:00, 14.77s/it, avg_loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Train avg loss: 1.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Val avg loss: 1.9536\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch26\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 27 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [10:04<00:00, 15.11s/it, avg_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Train avg loss: 1.9573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Val avg loss: 1.9496\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch27\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 28 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:53<00:00, 14.84s/it, avg_loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Train avg loss: 1.9470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Val avg loss: 1.9461\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch28\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 29 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:36<00:00, 14.42s/it, avg_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Train avg loss: 1.9436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Val avg loss: 1.9443\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch29\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exp[4,1e-05] Epoch 30 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [09:33<00:00, 14.35s/it, avg_loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Train avg loss: 1.9323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Val avg loss: 1.9437\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\epoch30\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-05_ep30\\best\n",
      "\n",
      ">>> å‚æ•°ç»„ [bs4_lr1e-05_ep30] è®­ç»ƒå®Œæˆ,æœ€ä½³éªŒè¯é›†loss: 1.9437ï¼Œæœ€ä¼˜æ¨¡å‹ä¿å­˜åœ¨: ./florence2-lora-bs4_lr1e-05_ep30\\best\n",
      "å›¾ç‰‡ C:\\Users\\taste\\Documents\\0_sis\\processed\\image_145.jpg ç”Ÿæˆæè¿°ï¼šA store front with a large sign reading \"PelquuerÃ­a Murano.\" The glass facade reflects the modern design of the store.\n"
     ]
    }
   ],
   "source": [
    "for params in param_grid:\n",
    "    BATCH_SIZE = params[\"BATCH_SIZE\"]\n",
    "    LEARNING_RATE = params[\"LEARNING_RATE\"]\n",
    "    EPOCHS = params[\"EPOCHS\"]\n",
    "    LOG_STEP = 10\n",
    "\n",
    "    print(f\"\\n===== å®éªŒå‚æ•°: BATCH_SIZE={BATCH_SIZE}, LR={LEARNING_RATE}, EPOCHS={EPOCHS} =====\")\n",
    "    # å‚æ•°ç»„å”¯ä¸€ç›®å½•\n",
    "    param_name = f\"bs{BATCH_SIZE}_lr{LEARNING_RATE:.0e}_ep{EPOCHS}\"\n",
    "    # æ¯ç»„å‚æ•°ä¸“å±æ–‡ä»¶å¤¹\n",
    "    param_dir = f\"./florence2-lora-{param_name}\"\n",
    "    os.makedirs(param_dir, exist_ok=True)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹ä¸ LoRA\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(CKPT, trust_remote_code=True).to(DEVICE)\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    num_training_steps = EPOCHS * len(train_dl)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_ckpt_dir = \"\"\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_losses = []\n",
    "        progress_bar = tqdm(enumerate(train_dl), total=len(train_dl), desc=f\"Exp[{BATCH_SIZE},{LEARNING_RATE}] Epoch {epoch+1} Train\")\n",
    "        for step, batch in progress_bar:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "            batch_losses.append(loss.item())\n",
    "            if (step + 1) % LOG_STEP == 0 or (step + 1) == len(train_dl):\n",
    "                avg_loss = sum(batch_losses[-LOG_STEP:]) / min(LOG_STEP, len(batch_losses[-LOG_STEP:]))\n",
    "                progress_bar.set_postfix({\"avg_loss\": avg_loss})\n",
    "        avg_epoch_loss = total_loss / len(train_dl)\n",
    "        print(f\"Epoch {epoch+1} - Train avg loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # éªŒè¯\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dl, desc=f\"Epoch {epoch+1} Val\"):\n",
    "                outputs = model(**batch)\n",
    "                val_loss += outputs.loss.item()\n",
    "        avg_val_loss = val_loss / len(val_dl)\n",
    "        print(f\"Epoch {epoch+1} - Val avg loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ==== è®°å½•åˆ° CSV ====\n",
    "        with open(loss_csv_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([param_name, epoch+1, avg_epoch_loss, avg_val_loss])\n",
    "\n",
    "        # === ä¿å­˜æ¯ä¸€è½® epoch checkpointï¼ˆå­æ–‡ä»¶å¤¹ï¼‰===\n",
    "        epoch_dir = os.path.join(param_dir, f\"epoch{epoch+1:02d}\")\n",
    "        os.makedirs(epoch_dir, exist_ok=True)\n",
    "        print(f\"ä¿å­˜æœ¬è½®æ¨¡å‹: {epoch_dir}\")\n",
    "        model.save_pretrained(epoch_dir)\n",
    "        processor.save_pretrained(epoch_dir)\n",
    "\n",
    "        # === ä¿å­˜ best checkpointï¼ˆæ¯ç»„ best å­æ–‡ä»¶å¤¹ï¼‰===\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_ckpt_dir = os.path.join(param_dir, \"best\")\n",
    "            model.save_pretrained(best_ckpt_dir)\n",
    "            processor.save_pretrained(best_ckpt_dir)\n",
    "            print(f\"ä¿å­˜æœ€ä¼˜æ¨¡å‹: {best_ckpt_dir}\")\n",
    "\n",
    "    print(f\"\\n>>> å‚æ•°ç»„ [{param_name}] è®­ç»ƒå®Œæˆ,æœ€ä½³éªŒè¯é›†loss: {best_val_loss:.4f}ï¼Œæœ€ä¼˜æ¨¡å‹ä¿å­˜åœ¨: {best_ckpt_dir}\")\n",
    "\n",
    "    # ==== æ¨ç†è¯„ä¼°ï¼ˆå¯é€‰ï¼‰====\n",
    "    infer_caption(model, processor, r\"C:\\Users\\taste\\Documents\\0_sis\\processed\\image_145.jpg\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5425bd",
   "metadata": {},
   "source": [
    "## åŠ è®­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be344d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½å‡flash_attn\n",
    "import sys, types, importlib.machinery\n",
    "\n",
    "def make_fake_mod(name):\n",
    "    mod = types.ModuleType(name)\n",
    "    mod.__spec__ = importlib.machinery.ModuleSpec(name, None)\n",
    "    return mod\n",
    "\n",
    "sys.modules[\"flash_attn\"] = make_fake_mod(\"flash_attn\")\n",
    "sys.modules[\"flash_attn.ops\"] = make_fake_mod(\"flash_attn.ops\")\n",
    "sys.modules[\"flash_attn.bert_flash_attention\"] = make_fake_mod(\"flash_attn.bert_flash_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c468f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taste\\Documents\\0_sis\\.florence2env2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, AdamW, get_scheduler\n",
    "from peft import PeftModel\n",
    "\n",
    "# === æ•°æ®ç±» ===\n",
    "class Florence2CaptionDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, image_dir):\n",
    "        self.samples = []\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    item = json.loads(line)\n",
    "                    self.samples.append(item)\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        image_path = os.path.join(self.image_dir, item[\"image\"])\n",
    "        image = Image.open(image_path)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        prompt = \"<DETAILED_CAPTION>\"\n",
    "        caption = item[\"caption\"]\n",
    "        return {\"prompt\": prompt, \"caption\": caption, \"image\": image}\n",
    "\n",
    "# === collate_fn ===\n",
    "def collate_fn(batch):\n",
    "    prompts = [b[\"prompt\"] for b in batch]\n",
    "    captions = [b[\"caption\"] for b in batch]\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    model_inputs = processor(\n",
    "        text=prompts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    labels = processor(\n",
    "        text=captions,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return {k: v.to(model.device) for k, v in model_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d9afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === æ¨ç†å‡½æ•°ï¼ˆå¯é€‰ï¼‰ ===\n",
    "def infer_caption(model, processor, img_path, device):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = \"<DETAILED_CAPTION>\"\n",
    "    inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            pixel_values=inputs.pixel_values,\n",
    "            max_new_tokens=80,\n",
    "            num_beams=3\n",
    "        )\n",
    "    output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(f\"å›¾ç‰‡ {img_path} ç”Ÿæˆæè¿°ï¼š{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42689a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === åŠ è½½æ•°æ® ===\n",
    "train_jsonl = r\"C:\\Users\\taste\\Documents\\0_sis\\captions_train.jsonl\"\n",
    "val_jsonl = r\"C:\\Users\\taste\\Documents\\0_sis\\captions_val.jsonl\"\n",
    "img_dir = r\"C:\\Users\\taste\\Documents\\0_sis\\processed\"\n",
    "\n",
    "train_ds = Florence2CaptionDataset(train_jsonl, img_dir)\n",
    "val_ds = Florence2CaptionDataset(val_jsonl, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e0a23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === è®­ç»ƒé…ç½® ===\n",
    "CKPT = \"microsoft/Florence-2-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "resume_ckpt_dir = \"./florence2-lora-bs4_lr1e-05_ep30/best\"  # ä½ çš„æœ€ä½³æ¨¡å‹è·¯å¾„\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-6\n",
    "EPOCHS = 15  # åŠ è®­è½®æ•°\n",
    "LOG_STEP = 10\n",
    "\n",
    "param_name = \"bs4_lr1e-5_ep30_plus15\"\n",
    "param_dir = f\"./florence2-lora-{param_name}\"\n",
    "os.makedirs(param_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5579625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ æ­£åœ¨åŠ è½½ LoRA é…ç½®...\n",
      "ğŸ“¥ åŠ è½½ base æ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taste\\Documents\\0_sis\\.florence2env2\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ æ³¨å…¥ LoRA...\n",
      "âœ… Trainable params: 233343944 / 233343944 (100.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\taste\\Documents\\0_sis\\.florence2env2\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === åŠ è½½æ¨¡å‹ ===\n",
    "#base_model = AutoModelForCausalLM.from_pretrained(CKPT, trust_remote_code=True).to(DEVICE)\n",
    "#model = PeftModel.from_pretrained(base_model, resume_ckpt_dir).to(DEVICE)\n",
    "#model.print_trainable_parameters()\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# === æ­£ç¡®åŠ è½½ LoRA çš„æ–¹å¼ ===\n",
    "print(\"ğŸ“¥ æ­£åœ¨åŠ è½½ LoRA é…ç½®...\")\n",
    "peft_config = PeftConfig.from_pretrained(resume_ckpt_dir)\n",
    "\n",
    "print(\"ğŸ“¥ åŠ è½½ base æ¨¡å‹...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "print(\"ğŸ“¥ æ³¨å…¥ LoRA...\")\n",
    "model = PeftModel.from_pretrained(base_model, resume_ckpt_dir)\n",
    "\n",
    "# === è½¬ä¸º GPU å¹¶è®¾ç½®è®­ç»ƒæ¨¡å¼ ===\n",
    "model = model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "# === ç¡®ä¿æ‰€æœ‰å‚æ•°æ˜¯å¯è®­ç»ƒçš„ï¼ˆå°¤å…¶æ˜¯ LoRA æ³¨å…¥åï¼‰===\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# === æ£€æŸ¥ trainable å‚æ•°æ•°é‡ ===\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if param.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"âœ… Trainable params: {trainable_params} / {total_params} ({100*trainable_params/total_params:.2f}%)\")\n",
    "\n",
    "\n",
    "# === åŠ è½½ Processor ===\n",
    "processor = AutoProcessor.from_pretrained(resume_ckpt_dir, trust_remote_code=True)\n",
    "\n",
    "# === DataLoader ===\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# === ä¼˜åŒ–å™¨ & Scheduler ===\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "num_training_steps = EPOCHS * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# === åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶ ===\n",
    "loss_csv_path = f\"train_val_loss_log_{param_name}.csv\"\n",
    "with open(loss_csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"param_group\", \"epoch\", \"train_loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae3f64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:27<00:00, 18.69s/it, avg_loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train avg loss: 1.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.34s/it]\n",
      "c:\\Users\\taste\\Documents\\0_sis\\.florence2env2\\lib\\site-packages\\peft\\utils\\save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val avg loss: 1.7784\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch01\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:29<00:00, 18.73s/it, avg_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train avg loss: 1.5142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val avg loss: 1.6940\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch02\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:21<00:00, 18.54s/it, avg_loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train avg loss: 1.3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val avg loss: 1.6607\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch03\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:22<00:00, 18.55s/it, avg_loss=1.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train avg loss: 1.1606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val avg loss: 1.6491\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch04\n",
      "ä¿å­˜æœ€ä¼˜æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:23<00:00, 18.59s/it, avg_loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train avg loss: 1.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val avg loss: 1.6626\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:17<00:00, 18.44s/it, avg_loss=0.886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train avg loss: 0.9302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val avg loss: 1.6546\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:16<00:00, 18.42s/it, avg_loss=0.843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train avg loss: 0.8546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val avg loss: 1.6757\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:19<00:00, 18.50s/it, avg_loss=0.802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train avg loss: 0.7681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val avg loss: 1.6796\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:22<00:00, 18.55s/it, avg_loss=0.652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train avg loss: 0.7120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val avg loss: 1.6966\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:21<00:00, 18.53s/it, avg_loss=0.646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train avg loss: 0.6565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val avg loss: 1.7137\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:43<00:00, 19.10s/it, avg_loss=0.661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train avg loss: 0.6189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Val avg loss: 1.7103\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:36<00:00, 18.92s/it, avg_loss=0.596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train avg loss: 0.5931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Val avg loss: 1.7217\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:34<00:00, 18.86s/it, avg_loss=0.553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train avg loss: 0.5660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Val avg loss: 1.7261\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:39<00:00, 18.99s/it, avg_loss=0.558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train avg loss: 0.5410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Val avg loss: 1.7283\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContinueTrain Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [12:42<00:00, 19.06s/it, avg_loss=0.537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train avg loss: 0.5249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Val avg loss: 1.7299\n",
      "ä¿å­˜æœ¬è½®æ¨¡å‹: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\epoch15\n",
      "\n",
      ">>> åŠ è®­å®Œæˆ, æœ€ä½³éªŒè¯é›† loss: 1.6491ï¼Œæ¨¡å‹ä¿å­˜åœ¨: ./florence2-lora-bs4_lr1e-5_ep30_plus15\\best\n",
      "å›¾ç‰‡ C:\\Users\\taste\\Documents\\0_sis\\processed\\image_145.jpg ç”Ÿæˆæè¿°ï¼šA modern shopfront with large glass windows and a white sign reading \"PelquerÃ­a Murano.\" The signage and branding indicate a motorcycle repair or replacement shop.\n"
     ]
    }
   ],
   "source": [
    "# === å¼€å§‹è®­ç»ƒ ===\n",
    "best_val_loss = float(\"inf\")\n",
    "best_ckpt_dir = \"\"\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_losses = []\n",
    "    progress_bar = tqdm(enumerate(train_dl), total=len(train_dl), desc=f\"ContinueTrain Epoch {epoch+1}\")\n",
    "    for step, batch in progress_bar:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        batch_losses.append(loss.item())\n",
    "        if (step + 1) % LOG_STEP == 0 or (step + 1) == len(train_dl):\n",
    "            avg_loss = sum(batch_losses[-LOG_STEP:]) / min(LOG_STEP, len(batch_losses[-LOG_STEP:]))\n",
    "            progress_bar.set_postfix({\"avg_loss\": avg_loss})\n",
    "    avg_epoch_loss = total_loss / len(train_dl)\n",
    "    print(f\"Epoch {epoch+1} - Train avg loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # === éªŒè¯ ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dl, desc=f\"Epoch {epoch+1} Val\"):\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "    avg_val_loss = val_loss / len(val_dl)\n",
    "    print(f\"Epoch {epoch+1} - Val avg loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # === è®°å½• loss ===\n",
    "    with open(loss_csv_path, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([param_name, epoch + 1, avg_epoch_loss, avg_val_loss])\n",
    "\n",
    "    # === ä¿å­˜æ¯è½® checkpoint ===\n",
    "    epoch_dir = os.path.join(param_dir, f\"epoch{epoch+1:02d}\")\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "    print(f\"ä¿å­˜æœ¬è½®æ¨¡å‹: {epoch_dir}\")\n",
    "    model.save_pretrained(epoch_dir)\n",
    "    processor.save_pretrained(epoch_dir)\n",
    "\n",
    "    # === ä¿å­˜æœ€ä¼˜ checkpoint ===\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_ckpt_dir = os.path.join(param_dir, \"best\")\n",
    "        model.save_pretrained(best_ckpt_dir)\n",
    "        processor.save_pretrained(best_ckpt_dir)\n",
    "        print(f\"ä¿å­˜æœ€ä¼˜æ¨¡å‹: {best_ckpt_dir}\")\n",
    "\n",
    "print(f\"\\n>>> åŠ è®­å®Œæˆ, æœ€ä½³éªŒè¯é›† loss: {best_val_loss:.4f}ï¼Œæ¨¡å‹ä¿å­˜åœ¨: {best_ckpt_dir}\")\n",
    "\n",
    "# === é€‰åšï¼šç”Ÿæˆä¸€ä¸ªæ ·æœ¬è¿›è¡Œæ¨ç† ===\n",
    "infer_caption(model, processor, r\"C:\\Users\\taste\\Documents\\0_sis\\processed\\image_145.jpg\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae28c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "# è·¯å¾„æ”¹ä¸ºä½ çš„æœ€æ–°å¾®è°ƒç›®å½•\n",
    "ckpt_dir = \"./florence2-lora-epoch5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(ckpt_dir, trust_remote_code=True).to(DEVICE)\n",
    "processor = AutoProcessor.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "637bbc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”Ÿæˆçš„æè¿°ï¼š The image shows a person walking past a Louis Barton store in London. The store has glass doors and a board with text on it.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = r\"C:\\Users\\taste\\Documents\\0_sis\\processed\\image_181.jpg\"  # æ¢æˆä½ æƒ³æµ‹è¯•çš„å›¾ç‰‡å\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "prompt = \"<DETAILED_CAPTION>\"\n",
    "\n",
    "inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        pixel_values=inputs.pixel_values,\n",
    "        max_new_tokens=80,\n",
    "        num_beams=3  # å¯ä»¥è¯•è¯•æ”¹ä¸º5ã€8ï¼Œç»“æœå¯èƒ½æ›´å¥½\n",
    "    )\n",
    "output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"ç”Ÿæˆçš„æè¿°ï¼š\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db0939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”Ÿæˆçš„æè¿°ï¼š The image shows a store front with a sign that reads \"Murano\" and a bicycle parked in front of it. There is a person sitting on a chair in the foreground, and a few plants in the background. The image is slightly blurred, giving it a dreamy quality.\n"
     ]
    }
   ],
   "source": [
    "img_path = r\"C:\\Users\\taste\\Documents\\0_sis\\processed\\image_145.jpg\"  # æ¢æˆä½ æƒ³æµ‹è¯•çš„å›¾ç‰‡å\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "prompt = \"<DETAILED_CAPTION>\"\n",
    "\n",
    "inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        pixel_values=inputs.pixel_values,\n",
    "        max_new_tokens=80,\n",
    "        num_beams=3  # å¯ä»¥è¯•è¯•æ”¹ä¸º5ã€8ï¼Œç»“æœå¯èƒ½æ›´å¥½\n",
    "    )\n",
    "output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"ç”Ÿæˆçš„æè¿°ï¼š\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".florence2env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
